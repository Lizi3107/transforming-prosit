{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abff2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cb8ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmnfs/home/l.mamisashvili/miniconda3/envs/prosit-t/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-22 15:18:00.461156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-22 15:18:01.224216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 15:18:11> /cmnfs/home/l.mamisashvili/miniconda3/envs/prosit-t/lib/python3.9/site-packages/peptdeep/mass_spec/ms_reader.py#L12: Cannot import `RawFileReader`, check if PythonNet is installed. See https://github.com/MannLabs/alphapeptdeep#pip\n"
     ]
    }
   ],
   "source": [
    "from peptdeep.model.generic_property_prediction import (\n",
    "    ModelInterface_for_Generic_AASeq_BinaryClassification,\n",
    "    ModelInterface_for_Generic_AASeq_Regression,\n",
    "    ModelInterface_for_Generic_ModAASeq_BinaryClassification,\n",
    "    ModelInterface_for_Generic_ModAASeq_Regression,\n",
    ")\n",
    "from peptdeep.model.generic_property_prediction import (\n",
    "    Model_for_Generic_AASeq_BinaryClassification_LSTM,\n",
    "    Model_for_Generic_AASeq_BinaryClassification_Transformer,\n",
    "    Model_for_Generic_AASeq_Regression_LSTM,\n",
    "    Model_for_Generic_AASeq_Regression_Transformer,\n",
    "    Model_for_Generic_ModAASeq_BinaryClassification_LSTM,\n",
    "    Model_for_Generic_ModAASeq_BinaryClassification_Transformer,\n",
    "    Model_for_Generic_ModAASeq_Regression_LSTM,\n",
    "    Model_for_Generic_ModAASeq_Regression_Transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "595f9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelInterface_for_Generic_ModAASeq_Regression(\n",
    "    model_class=Model_for_Generic_ModAASeq_Regression_Transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ce928f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': Model_for_Generic_ModAASeq_Regression_Transformer(\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (input_nn): AA_Mod_Embedding(\n",
       "     (mod_embedding): Mod_Embedding_FixFirstK(\n",
       "       (nn): Linear(in_features=103, out_features=2, bias=False)\n",
       "     )\n",
       "     (aa_embedding): Embedding(128, 248, padding_idx=0)\n",
       "   )\n",
       "   (hidden_nn): HFace_Transformer_with_PositionalEncoder(\n",
       "     (pos_encoder): PositionalEncoding()\n",
       "     (bert): Hidden_HFace_Transformer(\n",
       "       (bert): BertEncoder(\n",
       "         (layer): ModuleList(\n",
       "           (0): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "               (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (1): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "               (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (2): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "               (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (3): BertLayer(\n",
       "             (attention): BertAttention(\n",
       "               (self): BertSelfAttention(\n",
       "                 (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "               (output): BertSelfOutput(\n",
       "                 (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                 (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )\n",
       "             )\n",
       "             (intermediate): BertIntermediate(\n",
       "               (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "               (intermediate_act_fn): GELUActivation()\n",
       "             )\n",
       "             (output): BertOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "               (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (output_nn): Sequential(\n",
       "     (0): SeqAttentionSum(\n",
       "       (attn): Sequential(\n",
       "         (0): Linear(in_features=256, out_features=1, bias=False)\n",
       "         (1): Softmax(dim=1)\n",
       "       )\n",
       "     )\n",
       "     (1): PReLU(num_parameters=1)\n",
       "     (2): Dropout(p=0.1, inplace=False)\n",
       "     (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'optimizer': None,\n",
       " 'model_params': {'fixed_sequence_len': 0,\n",
       "  'min_pred_value': 0.0,\n",
       "  'dropout': 0.1,\n",
       "  'hidden_dim': 256,\n",
       "  'output_dim': 1,\n",
       "  'nlayers': 4},\n",
       " '_device_ids': [],\n",
       " '_device': device(type='cpu'),\n",
       " '_device_type': 'cpu',\n",
       " '_fixed_sequence_len': 0,\n",
       " '_min_pred_value': 0.0,\n",
       " 'loss_func': L1Loss(),\n",
       " '_target_column_to_predict': 'predicted_property',\n",
       " '_target_column_to_train': 'detected_property'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5094c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22ea56d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelInterface_for_Generic_ModAASeq_Regression' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m())\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelInterface_for_Generic_ModAASeq_Regression' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f3f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peptdeep.model.ms2 import ModelMS2Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "000fb5fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_frag_types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ms2 \u001b[38;5;241m=\u001b[39m ModelMS2Transformer(\u001b[43mnum_frag_types\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_frag_types' is not defined"
     ]
    }
   ],
   "source": [
    "ms2 = ModelMS2Transformer(num_frag_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecf30221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class SeqAttentionSum(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    apply linear transformation and tensor rescaling with softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 1, bias=False),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        attn = self.attn(x)\n",
    "        print(attn)\n",
    "        print(torch.sum(torch.mul(x, attn), dim=1))\n",
    "        return torch.sum(torch.mul(x, attn), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "873d32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = SeqAttentionSum(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99f64d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.2608e-01, 1.5943e-01, 2.5331e-01, 9.8373e-01, 7.4680e-01, 9.7373e-01,\n",
      "         5.6774e-01, 2.8373e-01, 4.6799e-01, 5.4813e-01, 4.0378e-01, 6.4741e-01,\n",
      "         1.3489e-01, 2.4661e-01, 7.7137e-01, 4.8373e-01, 4.7676e-01, 2.2019e-01,\n",
      "         7.8809e-02, 2.4353e-01, 6.7434e-01, 6.0718e-01, 3.4206e-01, 5.2088e-01,\n",
      "         2.4333e-01, 5.8480e-01, 4.7504e-02, 5.7012e-01, 3.1573e-01, 3.8020e-01,\n",
      "         7.6796e-01, 5.0488e-01, 2.2465e-01, 1.1290e-01, 3.2906e-01, 8.6623e-01,\n",
      "         3.1009e-02, 5.0773e-01, 9.1490e-01, 3.1279e-01, 1.2669e-01, 4.8335e-01,\n",
      "         6.6977e-01, 3.1997e-01, 9.9042e-01, 4.7055e-01, 4.0654e-01, 7.1775e-02,\n",
      "         4.2031e-01, 7.7907e-01],\n",
      "        [2.6743e-01, 3.3529e-02, 4.6635e-01, 7.9669e-01, 2.1487e-01, 3.6744e-01,\n",
      "         9.4360e-01, 7.2820e-01, 8.0573e-01, 5.2109e-01, 7.7740e-01, 7.6125e-01,\n",
      "         9.5645e-01, 2.4470e-01, 4.2113e-01, 3.6812e-01, 9.6741e-01, 6.0959e-01,\n",
      "         9.1718e-01, 8.8984e-01, 3.3928e-01, 8.4326e-01, 2.0324e-01, 2.0622e-01,\n",
      "         6.7317e-01, 2.4515e-01, 6.6693e-01, 8.0458e-01, 9.7394e-01, 6.0232e-01,\n",
      "         2.5884e-01, 5.8266e-01, 4.5492e-02, 8.6930e-01, 5.0923e-01, 8.2761e-04,\n",
      "         2.9994e-01, 1.1159e-01, 7.6405e-01, 8.6121e-01, 7.5635e-01, 3.8466e-01,\n",
      "         2.5254e-01, 3.8480e-01, 2.4006e-01, 8.5095e-02, 3.9451e-01, 3.4325e-01,\n",
      "         4.0641e-01, 7.4619e-01]])\n",
      "tensor([[1.],\n",
      "        [1.]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([23.2145, 25.9131], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "in_ =  torch.rand(2,50)\n",
    "t(in_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3f97db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.7500, 24.2620])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(in_, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ec514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
