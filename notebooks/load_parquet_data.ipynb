{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall dlomix prospect-dataset -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/wilhelm-lab/dlomix.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/wilhelm-lab/PROSPECT@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9551dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prospectdataset as prospect \n",
    "data_dir = \"./data\"\n",
    "pool_keyword = \"third_pool\"\n",
    "record_name = \"prospect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ab238",
   "metadata": {},
   "outputs": [],
   "source": [
    "prospect.download_dataset(record = record_name, task = \"all\",\n",
    "                          save_directory = data_dir, select_pool = pool_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3588fadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/TUM_third_pool_meta_data.parquet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# pick the path of the metadata file, can also be simply copied and pasted from previous cell outout \n",
    "#meta_data_filepath = './data/TUM_third_pool_meta_data.parquet'\n",
    "\n",
    "meta_data_filepath = glob.glob(os.path.join(data_dir, \"*\"+str(pool_keyword)+\"*meta_data.parquet\"))[0]\n",
    "meta_data_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf915104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['TUM_third_pool_1_01_01_annotation',\n",
       "  'TUM_third_pool_2_01_01_annotation',\n",
       "  'TUM_third_pool_3_01_01_annotation',\n",
       "  'TUM_third_pool_4_01_01_annotation',\n",
       "  'TUM_third_pool_5_01_01_annotation',\n",
       "  'TUM_third_pool_6_01_01_annotation'],\n",
       " ['./data/TUM_third_pool/TUM_third_pool_1_01_01_annotation.parquet',\n",
       "  './data/TUM_third_pool/TUM_third_pool_2_01_01_annotation.parquet',\n",
       "  './data/TUM_third_pool/TUM_third_pool_3_01_01_annotation.parquet',\n",
       "  './data/TUM_third_pool/TUM_third_pool_4_01_01_annotation.parquet',\n",
       "  './data/TUM_third_pool/TUM_third_pool_5_01_01_annotation.parquet',\n",
       "  './data/TUM_third_pool/TUM_third_pool_6_01_01_annotation.parquet'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "annotation_dirs = [path for path in glob.glob(os.path.join(data_dir, \"*\"+str(pool_keyword)+\"*\")) if os.path.isdir(path)]\n",
    "annotations_filepaths = [glob.glob(os.path.join(d, \"*.parquet\")) for d in annotation_dirs]\n",
    "annotations_filepaths = list(itertools.chain(*annotations_filepaths))\n",
    "annotations_names = [Path(f).stem for f in annotations_filepaths]\n",
    "\n",
    "annotations_names, annotations_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fa8049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': './data/TUM_third_pool_meta_data.parquet',\n",
       " 'annotations': {'third_pool': {'TUM_third_pool_1_01_01_annotation': './data/TUM_third_pool/TUM_third_pool_1_01_01_annotation.parquet',\n",
       "   'TUM_third_pool_2_01_01_annotation': './data/TUM_third_pool/TUM_third_pool_2_01_01_annotation.parquet',\n",
       "   'TUM_third_pool_3_01_01_annotation': './data/TUM_third_pool/TUM_third_pool_3_01_01_annotation.parquet',\n",
       "   'TUM_third_pool_4_01_01_annotation': './data/TUM_third_pool/TUM_third_pool_4_01_01_annotation.parquet',\n",
       "   'TUM_third_pool_5_01_01_annotation': './data/TUM_third_pool/TUM_third_pool_5_01_01_annotation.parquet',\n",
       "   'TUM_third_pool_6_01_01_annotation': './data/TUM_third_pool/TUM_third_pool_6_01_01_annotation.parquet'}},\n",
       " 'parameters': {'target_column_key': 'intensities_raw'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_dict = {\n",
    "    \"metadata\": meta_data_filepath,\n",
    "    \"annotations\": {\n",
    "        pool_keyword: dict(zip(annotations_names, annotations_filepaths))\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"target_column_key\": \"intensities_raw\"\n",
    "    }\n",
    "}\n",
    "\n",
    "input_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c22399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# later we can feed the dict directly as a data source, for now we stick to json format\n",
    "\n",
    "import json\n",
    "with open(\"input_config.json\", 'w') as fp:\n",
    "    json.dump(input_data_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d1015",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 10:58:33.998247: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 10:58:34.107662: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-14 10:58:34.752297: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-14 10:58:34.752367: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-14 10:58:34.752372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optionally Downloading and processing the data...\n",
      "Annotations directory:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool\n",
      "Metadata filepath:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool_meta_data.parquet\n",
      "Base directory:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks\n",
      "--------------------------------------------------------------------------------\n",
      "Starting processing and filtering the pool, this may take a while...\n",
      "--------------------------------------------------------------------------------\n",
      "Reading metadata file from /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool_meta_data.parquet\n",
      "Reading and processing annotation files...\n",
      "Reading file:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool/TUM_third_pool_1_01_01_annotation.parquet\n",
      "Filtering annotation file...\n",
      "Sorting by fragment_score...\n",
      "Dropping duplicates...\n",
      "Sorting by intensity...\n",
      "Dropping duplicates...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Reading file:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool/TUM_third_pool_2_01_01_annotation.parquet\n",
      "Filtering annotation file...\n",
      "Sorting by fragment_score...\n",
      "Dropping duplicates...\n",
      "Sorting by intensity...\n",
      "Dropping duplicates...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Reading file:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool/TUM_third_pool_3_01_01_annotation.parquet\n",
      "Filtering annotation file...\n",
      "Sorting by fragment_score...\n",
      "Dropping duplicates...\n",
      "Sorting by intensity...\n",
      "Dropping duplicates...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Reading file:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool/TUM_third_pool_4_01_01_annotation.parquet\n",
      "Filtering annotation file...\n",
      "Sorting by fragment_score...\n",
      "Dropping duplicates...\n",
      "Sorting by intensity...\n",
      "Dropping duplicates...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Reading file:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool/TUM_third_pool_5_01_01_annotation.parquet\n",
      "Filtering annotation file...\n",
      "Sorting by fragment_score...\n",
      "Dropping duplicates...\n",
      "Sorting by intensity...\n",
      "Dropping duplicates...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Reading file:  /cmnfs/home/l.mamisashvili/transforming-prosit/notebooks/./data/TUM_third_pool/TUM_third_pool_6_01_01_annotation.parquet\n",
      "Filtering annotation file...\n",
      "Sorting by fragment_score...\n",
      "Dropping duplicates...\n",
      "Sorting by intensity...\n",
      "Dropping duplicates...\n",
      "Done.\n",
      "--------------------------------------------------------------------------------\n",
      "Building annotation dataframe...\n",
      "Grouping annotation by scan number and raw file...\n",
      "Grouping metadata by scan number and raw file...\n"
     ]
    }
   ],
   "source": [
    "from dlomix.data import IntensityDataset\n",
    "from dlomix.data.feature_extractors import (\n",
    "    ModificationGainFeature,\n",
    "    ModificationLocationFeature,\n",
    "    ModificationLossFeature,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "int_data = IntensityDataset(\n",
    "    data_source=\"input_config.json\",\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_ratio=0.15,\n",
    "    precursor_charge_col=\"precursor_charge_onehot\",\n",
    "    sequence_col=\"modified_sequence\",\n",
    "    collision_energy_col=\"collision_energy_aligned_normed\",\n",
    "    intensities_col=\"intensities_raw\",\n",
    "    features_to_extract=[\n",
    "        ModificationLocationFeature(),\n",
    "        ModificationLossFeature(),\n",
    "        ModificationGainFeature(),\n",
    "    ],\n",
    "    parser=\"proforma\",\n",
    "    metadata_filtering_criteria = {\n",
    "        \"peptide_length\": f\"<= {SEQ_LENGTH}\",\n",
    "        \"precursor_charge\": \"<= 6\",\n",
    "        \"fragmentation\": \"== 'HCD'\",\n",
    "#         \"mass_analyzer\": \"== 'FTMS'\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b305812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Training examples\", BATCH_SIZE * len(int_data.train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Validation examples\", BATCH_SIZE * len(int_data.val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823679f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
