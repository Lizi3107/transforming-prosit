{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ac36ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:38:03.805049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-11 16:38:04.544293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from dlomix.constants import ALPHABET_UNMOD\n",
    "from prosit_t.layers import (\n",
    "    MetaEncoder,\n",
    "    FusionLayer,\n",
    "    RegressorV2,\n",
    "    PositionalEmbedding,\n",
    "    TransformerEncoder,\n",
    "    CrossAttention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bd032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4263300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52bb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee81384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 30\n",
    "embeddings_count = len(ALPHABET_UNMOD) + 2\n",
    "max_ion = SEQ_LENGTH - 1\n",
    "len_fion = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ba91d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output_dim = 64\n",
    "num_heads = 16\n",
    "ff_dim = 32\n",
    "transformer_dropout = 0.1\n",
    "num_transformers = 4\n",
    "dropout_rate = 0\n",
    "dense_dim_factor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47ca2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-11 16:38:06.623745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38136 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:21:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " string_lookup (StringLooku  (None, 30)                   0         ['input_1[0][0]']             \n",
      " p)                                                                                               \n",
      "                                                                                                  \n",
      " meta_encoder (MetaEncoder)  (None, 256)                  2048      ['input_2[0][0]',             \n",
      "                                                                     'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " positional_embedding (Posi  (None, 30, 64)               1408      ['string_lookup[0][0]']       \n",
      " tionalEmbedding)                                                                                 \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 256)               0         ['meta_encoder[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, 30, 64)               1078912   ['positional_embedding[0][0]']\n",
      " formerEncoder)                                                                                   \n",
      "                                                                                                  \n",
      " cross_attention (CrossAtte  (None, 1, 256)               2634496   ['reshape[0][0]',             \n",
      " ntion)                                                              'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 256)                  0         ['cross_attention[0][0]']     \n",
      "                                                                                                  \n",
      " regressor_v2 (RegressorV2)  (None, 174)                  44718     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3761582 (14.35 MB)\n",
      "Trainable params: 3761582 (14.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_sequence = tf.keras.layers.Input(shape=(30,))\n",
    "in_collision_energy = tf.keras.layers.Input(shape=(6,))\n",
    "in_precursor_charge = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "encoded_meta = MetaEncoder(embedding_output_dim*dense_dim_factor, dropout_rate)([in_collision_energy, in_precursor_charge])\n",
    "encoded_meta = tf.keras.layers.Reshape([1, embedding_output_dim*dense_dim_factor])(encoded_meta)\n",
    "x = preprocessing.StringLookup(vocabulary=list(ALPHABET_UNMOD.keys()))(in_sequence)\n",
    "x = PositionalEmbedding(\n",
    "            embeddings_count, embedding_output_dim\n",
    "        )(x)\n",
    "x = TransformerEncoder(\n",
    "            embedding_output_dim, num_heads, ff_dim, rate=transformer_dropout, num_transformers=num_transformers\n",
    "        )(x)\n",
    "x = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_output_dim * dense_dim_factor,\n",
    "            dropout=dropout_rate)(x=encoded_meta, context=x) #x is query, econded meta - key,value\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = RegressorV2(len_fion * max_ion)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[in_sequence, in_collision_energy, in_precursor_charge],outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04aceea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_70 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " string_lookup_22 (StringLo  (None, 30)                   0         ['input_70[0][0]']            \n",
      " okup)                                                                                            \n",
      "                                                                                                  \n",
      " positional_embedding_22 (P  (None, 30, 64)               1408      ['string_lookup_22[0][0]']    \n",
      " ositionalEmbedding)                                                                              \n",
      "                                                                                                  \n",
      " transformer_encoder_22 (Tr  (None, 30, 64)               1078912   ['positional_embedding_22[0][0\n",
      " ansformerEncoder)                                                  ]']                           \n",
      "                                                                                                  \n",
      " input_71 (InputLayer)       [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_72 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " flatten_25 (Flatten)        (None, 1920)                 0         ['transformer_encoder_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " meta_encoder_23 (MetaEncod  (None, 256)                  2048      ['input_71[0][0]',            \n",
      " er)                                                                 'input_72[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 2176)                 0         ['flatten_25[0][0]',          \n",
      " )                                                                   'meta_encoder_23[0][0]']     \n",
      "                                                                                                  \n",
      " dense_134 (Dense)           (None, 512)                  1114624   ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " regressor_v2_18 (Regressor  (None, 174)                  89262     ['dense_134[0][0]']           \n",
      " V2)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2286254 (8.72 MB)\n",
      "Trainable params: 2286254 (8.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_sequence = tf.keras.layers.Input(shape=(30,))\n",
    "in_collision_energy = tf.keras.layers.Input(shape=(6,))\n",
    "in_precursor_charge = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "encoded_meta = MetaEncoder(embedding_output_dim*dense_dim_factor, dropout_rate)([in_collision_energy, in_precursor_charge])\n",
    "# encoded_meta = tf.keras.layers.Reshape([1, embedding_output_dim*dense_dim_factor])(encoded_meta)\n",
    "x = preprocessing.StringLookup(vocabulary=list(ALPHABET_UNMOD.keys()))(in_sequence)\n",
    "x = PositionalEmbedding(\n",
    "            embeddings_count, embedding_output_dim\n",
    "        )(x)\n",
    "x = TransformerEncoder(\n",
    "            embedding_output_dim, num_heads, ff_dim, rate=transformer_dropout, num_transformers=num_transformers\n",
    "        )(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Concatenate()([x, encoded_meta])\n",
    "x = tf.keras.layers.Dense(512)(x)\n",
    "x = RegressorV2(len_fion * max_ion)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[in_sequence, in_collision_energy, in_precursor_charge],outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95a2e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)       [(None, 30)]                 0         []                            \n",
      "                                                                                                  \n",
      " string_lookup_5 (StringLoo  (None, 30)                   0         ['input_16[0][0]']            \n",
      " kup)                                                                                             \n",
      "                                                                                                  \n",
      " positional_embedding_5 (Po  (None, 30, 64)               1408      ['string_lookup_5[0][0]']     \n",
      " sitionalEmbedding)                                                                               \n",
      "                                                                                                  \n",
      " transformer_encoder_5 (Tra  (None, 30, 64)               1078912   ['positional_embedding_5[0][0]\n",
      " nsformerEncoder)                                                   ']                            \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)         (None, 1920)                 0         ['transformer_encoder_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)       [(None, 6)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_52 (Dense)            (None, 256)                  491776    ['flatten_8[0][0]']           \n",
      "                                                                                                  \n",
      " meta_encoder_5 (MetaEncode  (None, 256)                  2048      ['input_17[0][0]',            \n",
      " r)                                                                  'input_18[0][0]']            \n",
      "                                                                                                  \n",
      " attention_4 (Attention)     (None, 256)                  0         ['dense_52[0][0]',            \n",
      "                                                                     'meta_encoder_5[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)         (None, 256)                  0         ['attention_4[0][0]']         \n",
      "                                                                                                  \n",
      " regressor_v2_5 (RegressorV  (None, 174)                  44718     ['flatten_9[0][0]']           \n",
      " 2)                                                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1618862 (6.18 MB)\n",
      "Trainable params: 1618862 (6.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_sequence = tf.keras.layers.Input(shape=(30,))\n",
    "in_collision_energy = tf.keras.layers.Input(shape=(6,))\n",
    "in_precursor_charge = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "encoded_meta = MetaEncoder(embedding_output_dim*dense_dim_factor, dropout_rate)([in_collision_energy, in_precursor_charge])\n",
    "x = preprocessing.StringLookup(vocabulary=list(ALPHABET_UNMOD.keys()))(in_sequence)\n",
    "x = PositionalEmbedding(\n",
    "            embeddings_count, embedding_output_dim\n",
    "        )(x)\n",
    "x = TransformerEncoder(\n",
    "            embedding_output_dim, num_heads, ff_dim, rate=transformer_dropout, num_transformers=num_transformers\n",
    "        )(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(embedding_output_dim * dense_dim_factor)(x)\n",
    "x = tf.keras.layers.Attention()([x,encoded_meta]) #x is query, econded meta - key,value\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = RegressorV2(len_fion * max_ion)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[in_sequence, in_collision_energy, in_precursor_charge],outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd17833",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sequence = tf.keras.layers.Input(shape=(30,))\n",
    "in_collision_energy = tf.keras.layers.Input(shape=(6,))\n",
    "in_precursor_charge = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "encoded_meta = MetaEncoder(embedding_output_dim*dense_dim_factor, dropout_rate)([in_collision_energy, in_precursor_charge])\n",
    "\n",
    "x = preprocessing.StringLookup(vocabulary=list(ALPHABET_UNMOD.keys()))(in_sequence)\n",
    "x = PositionalEmbedding(\n",
    "            embeddings_count, embedding_output_dim\n",
    "        )(x)\n",
    "x = TransformerEncoder(\n",
    "            embedding_output_dim, num_heads, ff_dim, rate=transformer_dropout, num_transformers=num_transformers\n",
    "        )(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(embedding_output_dim*dense_dim_factor)(x)\n",
    "x = tf.keras.layers.Multiply()([x, encoded_meta])\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = RegressorV2(len_fion * max_ion)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[in_sequence, in_collision_energy, in_precursor_charge],outputs=x)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
